{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL PROTOTYPE \n",
    "\n",
    "- Alejandro Giorgio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://data.sfgov.org/resource/wg3w-h783.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize offset\n",
    "offset = 0\n",
    "\n",
    "# Number of records to fetch in each request\n",
    "limit = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La API tiene un límite de devolver 50000 filas de datos por vez. Si queremos obtener más de 50000 registros, tenemos que manejar esta limitación en nuestro código.\n",
    "\n",
    "Podemos usar los parámetros ‘$offset’ y ‘$limit’ que provee la API para obtener todos los registros. Empezamos con un offset de 0 y un límite de 50000 (o cualquier otro número hasta el límite máximo permitido por la API). Enviamos un pedido GET a la API con estos parámetros y obtenemos los primeros 50000 registros. Después aumentamos el offset por el límite (50000 en este caso) y enviamos otro pedido para obtener los siguientes 50000 registros. Repetimos este proceso hasta que la API no devuelva datos, lo que significa que hemos obtenido todos los registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Construct the URL with the offset and limit parameters\n",
    "    url = f\"{base_url}?$limit={limit}&$offset={offset}\"\n",
    "\n",
    "    # Send a GET request to the API endpoint\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Convert the response content to a pandas DataFrame\n",
    "        data = pd.read_json(io.StringIO(response.text))\n",
    "\n",
    "        # If no data is returned, we've fetched all records\n",
    "        if data.empty:\n",
    "            break\n",
    "\n",
    "        # Append the data to df DataFrame using pandas.concat\n",
    "        df = pd.concat([df, data], ignore_index=True)\n",
    "\n",
    "        # Increase the offset by limit\n",
    "        offset += limit\n",
    "\n",
    "        print(f\"offset is {offset}\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch data. Status code: {response.status_code}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime\n",
    "from constants import date_columns, string_columns\n",
    "\n",
    "for col in date_columns:\n",
    "    df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "for col in string_columns:\n",
    "    df[col] = df[col].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = int(0.6 * len(df))\n",
    "df = df.dropna(thresh=threshold, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.replace(':', '')\n",
    "df.columns = df.columns.str.replace('@', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['point'] = df['point'].apply(json.dumps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTEGRATION WITH REDSHIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your Redshift credentials\n",
    "host = os.environ[\"REDSHIFT_HOST\"]\n",
    "dbname = os.environ[\"REDSHIFT_DATABASE\"]\n",
    "user = os.environ[\"REDSHIFT_USER\"]\n",
    "password = os.environ[\"REDSHIFT_PWD\"]\n",
    "port = os.environ[\"REDSHIFT_PORT\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a connection to Redshift\n",
    "conn = psycopg2.connect(\n",
    "    dbname=dbname,\n",
    "    user=user,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    host=host\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that every single operation with the database is treated as a transaction and is immediately committed to the database.\n",
    "conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "\n",
    "# Create a cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Creating the table\n",
    "create_table_command = \"\"\"\n",
    "CREATE TABLE sf_police_incidents (\n",
    "    incident_datetime TIMESTAMP,\n",
    "    incident_date DATE,\n",
    "    incident_time TIME,\n",
    "    incident_year INT,\n",
    "    incident_day_of_week VARCHAR(255),\n",
    "    report_datetime TIMESTAMP,\n",
    "    row_id BIGINT,\n",
    "    incident_id INT,\n",
    "    incident_number INT,\n",
    "    report_type_code VARCHAR(255),\n",
    "    report_type_description VARCHAR(255),\n",
    "    incident_code INT,\n",
    "    incident_category VARCHAR(255),\n",
    "    incident_subcategory VARCHAR(255),\n",
    "    incident_description VARCHAR(255),\n",
    "    resolution VARCHAR(255),\n",
    "    police_district VARCHAR(255),\n",
    "    cad_number FLOAT,\n",
    "    intersection VARCHAR(255),\n",
    "    cnn FLOAT,\n",
    "    analysis_neighborhood VARCHAR(255),\n",
    "    supervisor_district FLOAT,\n",
    "    supervisor_district_2012 FLOAT,\n",
    "    latitude FLOAT,\n",
    "    longitude FLOAT,\n",
    "    point VARCHAR(255),\n",
    "    computed_region_26cr_cadq FLOAT,\n",
    "    computed_region_qgnn_b9vv FLOAT,\n",
    "    computed_region_jwn9_ihcz FLOAT\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL command\n",
    "cur.execute(create_table_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# Create an SQLAlchemy engine\n",
    "engine = create_engine(\n",
    "    f\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}\"\n",
    ")\n",
    "\n",
    "# Write the DataFrame to the table in Redshift\n",
    "df.to_sql(\"sf_police_incidents\", engine, if_exists=\"append\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
